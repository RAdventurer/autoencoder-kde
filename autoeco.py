import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, UpSampling2D
from keras.preprocessing.image import ImageDataGenerator
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import random
from sklearn.neighbors import KernelDensity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import glob

# Check GPU availability
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

SIZE = 224
batch_size = 64

datagen = ImageDataGenerator(rescale=1./255)

train_generator = datagen.flow_from_directory(
    'datasets/train',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input',
    classes=['OK'],
    shuffle=True
)

validation_generator = datagen.flow_from_directory(
    'datasets/val',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input',
    classes=['OK'],
    shuffle=False
)
test_generator = datagen.flow_from_directory(
    'datasets/test',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input',
    classes=['OK'],
    shuffle=False
)
anomaly_generator = datagen.flow_from_directory(
    'datasets/train',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input',
    classes=['NG'],
    shuffle=False
)

print(f"Number of training samples: {len(train_generator)}")
print(f"Number of validation samples: {len(validation_generator)}")

# Define the autoencoder model

# Encoder
model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))

# Decoder
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))

model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])
model.summary()

# Fit the model
history = model.fit(
        train_generator,
        epochs=1000,
        validation_data=validation_generator,
        shuffle=True
)

# Plot the training and validation loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Get all batches generated by the datagen and pick a batch for prediction
data_batch = []  # Capture all training batches as a numpy array
img_num = 0
while img_num <= train_generator.batch_index:  # gets each generated batch of size batch_size
    data = train_generator.next()
    data_batch.append(data[0])
    img_num += 1

predicted = model.predict(data_batch[0])  # Predict on the first batch of images

# Sanity check, view few images and corresponding reconstructions
image_number = random.randint(0, predicted.shape[0])
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(data_batch[0][image_number])
plt.subplot(122)
plt.imshow(predicted[image_number])
plt.show()

# Let us examine the reconstruction error between our validation data (good/normal images) and the anomaly images
validation_error = model.evaluate(test_generator)
anomaly_error = model.evaluate(anomaly_generator)

print("Reconstruction error for the validation (normal) data is: ", validation_error)
print("Reconstruction error for the anomaly data is: ", anomaly_error)

# Extract the encoder network with trained weights for KDE
encoder_model = Sequential()
encoder_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3), weights=model.layers[0].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', weights=model.layers[2].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', weights=model.layers[4].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(8, (3, 3), activation='relu', padding='same', weights=model.layers[6].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(8, (3, 3), activation='tanh', padding='same', weights=model.layers[8].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.summary()

# Calculate KDE using sklearn
encoded_images = encoder_model.predict(train_generator)

# Flatten the encoder output because KDE from sklearn takes 1D vectors as input
encoder_output_shape = encoder_model.output_shape
out_vector_shape = encoder_output_shape[1]*encoder_output_shape[2]*encoder_output_shape[3]

encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]

# Fit KDE to the image latent data
kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(encoded_images_vector)

# Calculate density and reconstruction error to find their means values for good and anomaly images
def calc_density_and_recon_error(batch_images):
    density_list = []
    recon_error_list = []
    for im in range(0, batch_images.shape[0] - 1):
        img = batch_images[im]
        img = img[np.newaxis, :, :, :]
        encoded_img = encoder_model.predict(img)
        encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img]
        density = kde.score_samples(encoded_img)[0]
        reconstruction = model.predict(img)
        reconstruction_error = model.evaluate(reconstruction, img, batch_size=1)[0]
        density_list.append(density)
        recon_error_list.append(reconstruction_error)
    
    average_density = np.mean(np.array(density_list))
    stdev_density = np.std(np.array(density_list))
    average_recon_error = np.mean(np.array(recon_error_list))
    stdev_recon_error = np.std(np.array(recon_error_list))
    
    return average_density, stdev_density, average_recon_error, stdev_recon_error

train_batch = train_generator.next()[0]
anomaly_batch = anomaly_generator.next()[0]

uninfected_values = calc_density_and_recon_error(train_batch)
infected_values = calc_density_and_recon_error(anomaly_batch)

print("Average density (non-anomalous images): ", uninfected_values[0])
print("Standard deviation density (non-anomalous images): ", uninfected_values[1])
print("Average reconstruction error (non-anomalous images): ", uninfected_values[2])
print("Standard deviation reconstruction error (non-anomalous images): ", uninfected_values[3])

print("Average density (anomalous images): ", infected_values[0])
print("Standard deviation density (anomalous images): ", infected_values[1])
print("Average reconstruction error (anomalous images): ", infected_values[2])
print("Standard deviation reconstruction error (anomalous images): ", infected_values[3])








# Collect all images from the anomaly_generator
def collect_images_from_generator(generator):
    all_images = []
    for _ in range(len(generator)):
        batch_images = generator.next()[0]
        all_images.extend(batch_images)
    return np.array(all_images)

anomaly_images = collect_images_from_generator(anomaly_generator)
# Collect all images from the validation_generator
validation_images = collect_images_from_generator(test_generator)
def predict_on_images(images):
    densities = []
    recon_errors = []
    predictions = []
    
    for img in images:
        img = img[np.newaxis, :, :, :]
        encoded_img = encoder_model.predict(img)
        encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img]
        density = kde.score_samples(encoded_img)[0]
        reconstruction = model.predict(img)
        reconstruction_error = np.mean(np.square(reconstruction - img))  # Error calculation
        
        densities.append(density)
        recon_errors.append(reconstruction_error)
        
        #if density < uninfected_values[0] - uninfected_values[1] ''' or reconstruction_error > uninfected_values[2] + uninfected_values[3]''':
        if density < uninfected_values[0] - uninfected_values[1] :
            predictions.append(1)  # Anomaly
        else:
            predictions.append(0)  # Normal
    
    return np.array(predictions), np.array(densities), np.array(recon_errors)

# Predict on all validation and anomaly images
validation_predictions, validation_densities, validation_errors = predict_on_images(validation_images)
anomaly_predictions, anomaly_densities, anomaly_errors = predict_on_images(anomaly_images)
# Create true labels for the predictions
true_values_validation = np.zeros(len(validation_predictions))
true_values_anomaly = np.ones(len(anomaly_predictions))

# Combine predictions and true values
predictions_combined = np.concatenate([validation_predictions, anomaly_predictions])
true_values_combined = np.concatenate([true_values_validation, true_values_anomaly])
print("predictions_combined:\n", sum(predictions_combined),sum(validation_predictions),sum(anomaly_predictions))

# Evaluation metrics
print("Confusion Matrix:\n", confusion_matrix(true_values_combined, predictions_combined))
print("Classification Report:\n", classification_report(true_values_combined, predictions_combined))
print("Accuracy Score: ", accuracy_score(true_values_combined, predictions_combined))
print("Precision Score: ", precision_score(true_values_combined, predictions_combined))
print("Recall Score: ", recall_score(true_values_combined, predictions_combined))
print("F1 Score: ", f1_score(true_values_combined, predictions_combined))




import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

# Define your functions and existing code here
# calc_density_and_recon_error, predict_images, etc.

# Visualize a maximum of 30 images for each class
def visualize_predictions(images, predictions, densities, errors, true_labels, class_names, max_per_class=9):
    # Convert inputs to lists if they are not already
    images = list(images)
    predictions = list(predictions)
    densities = list(densities)
    errors = list(errors)
    true_labels = list(true_labels)
    
    # Filter out a maximum of max_per_class images for each class
    filtered_images = []
    filtered_predictions = []
    filtered_densities = []
    filtered_errors = []
    filtered_true_labels = []
    
    for class_label in range(len(class_names)):
        class_indices = [i for i, label in enumerate(true_labels) if label == class_label]
        class_indices = class_indices[:max_per_class]
        
        filtered_images.extend([images[i] for i in class_indices])
        filtered_predictions.extend([predictions[i] for i in class_indices])
        filtered_densities.extend([densities[i] for i in class_indices])
        filtered_errors.extend([errors[i] for i in class_indices])
        filtered_true_labels.extend([true_labels[i] for i in class_indices])
    
    num_images = len(filtered_images)
    num_cols = 3
    num_rows = (num_images + num_cols - 1) // num_cols  # Ceiling division
    plt.figure(figsize=(15, num_rows * 4))  # Adjust the height based on the number of rows
    
    for i in range(num_images):
        plt.subplot(num_rows, num_cols, i + 1)
        plt.imshow(filtered_images[i])
        pred_class = int(filtered_predictions[i])
        true_class = int(filtered_true_labels[i])
        plt.title(f"Pred: {class_names[pred_class]}\nTrue: {class_names[true_class]}\nDensity: {filtered_densities[i]:.2f}\nError: {filtered_errors[i]:.2f}")
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()
# Visualize anomaly images
visualize_predictions(anomaly_images, anomaly_predictions, anomaly_densities, anomaly_errors, true_values_anomaly, class_names=["Normal", "Anomaly"])


# Visualize validation images
visualize_predictions(validation_images, validation_predictions, validation_densities, validation_errors, true_values_validation, class_names=["Normal", "Anomaly"])






















'''

# Generate final prediction for validation and anomaly images based on reconstruction error and density of the encoded images
def predict_images(batch_images, avg_density, std_density, avg_recon_error, std_recon_error):
    predictions = []
    density_list = []
    recon_error_list = []
    for im in range(0, batch_images.shape[0] - 1):
        img = batch_images[im]
        img = img[np.newaxis, :, :, :]
        encoded_img = encoder_model.predict(img)
        encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img]
        density = kde.score_samples(encoded_img)[0]
        reconstruction = model.predict(img)
        reconstruction_error = model.evaluate(reconstruction, img, batch_size=1)[0]
        density_list.append(density)
        recon_error_list.append(reconstruction_error)
       # if density < avg_density - std_density or reconstruction_error > avg_recon_error + std_recon_error:
        if density < avg_density - std_density :

            predictions.append(1)
        else:
            predictions.append(0)
    
    return predictions, density_list, recon_error_list

validation_batch = validation_generator.next()[0]
anomaly_batch = anomaly_generator.next()[0]

validation_predictions, validation_densities, validation_errors = predict_images(validation_batch, 
                                                                    uninfected_values[0], 
                                                                    uninfected_values[1], 
                                                                    uninfected_values[2], 
                                                                    uninfected_values[3])

anomaly_predictions, anomaly_densities, anomaly_errors = predict_images(anomaly_batch, 
                                                                    uninfected_values[0], 
                                                                    uninfected_values[1], 
                                                                    uninfected_values[2], 
                                                                    uninfected_values[3])

# Create true values for the predictions
true_values_validation = np.zeros(len(validation_predictions))
true_values_anomaly = np.ones(len(anomaly_predictions))

# Combining the results and calculating evaluation metrics
predictions_combined = np.concatenate([validation_predictions, anomaly_predictions])
true_values_combined = np.concatenate([true_values_validation, true_values_anomaly])

print("Confusion Matrix:\n", confusion_matrix(true_values_combined, predictions_combined))
print("Classification Report:\n", classification_report(true_values_combined, predictions_combined))
print("Accuracy Score: ", accuracy_score(true_values_combined, predictions_combined))
print("Precision Score: ", precision_score(true_values_combined, predictions_combined))
print("Recall Score: ", recall_score(true_values_combined, predictions_combined))
print("F1 Score: ", f1_score(true_values_combined, predictions_combined))



'''












import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Assume encoded_images_vector is your dataset
# Normalize and reduce dimensions for visualization
scaler = StandardScaler()
encoded_images_scaled = scaler.fit_transform(encoded_images_vector)
pca = PCA(n_components=2)
encoded_images_pca = pca.fit_transform(encoded_images_scaled)

# Define different parameters for KDE
kernels = ['gaussian', 'tophat', 'epanechnikov', 'linear', 'cosine', 'exponential']
bandwidths = [0.1, 0.5, 1.0, 2.0, 3.0 , 100]

# Function to plot KDE results
def plot_kde_results(encoded_images_pca, kde_results, bandwidths, kernels):
    plt.figure(figsize=(15, 10))
    num_kernels = len(kernels)
    num_bandwidths = len(bandwidths)
    
    for i, kernel in enumerate(kernels):
        for j, bandwidth in enumerate(bandwidths):
            plt.subplot(num_kernels, num_bandwidths, i * num_bandwidths + j + 1)
            kde = kde_results[(kernel, bandwidth)]
            x = np.linspace(encoded_images_pca[:, 0].min(), encoded_images_pca[:, 0].max(), 1000)
            y = np.linspace(encoded_images_pca[:, 1].min(), encoded_images_pca[:, 1].max(), 1000)
            X, Y = np.meshgrid(x, y)
            xy_samples = np.vstack([X.ravel(), Y.ravel()]).T
            Z = np.exp(kde.score_samples(xy_samples))
            Z = Z.reshape(X.shape)
            plt.contourf(X, Y, Z, levels=20, cmap='viridis')
            plt.title(f'Kernel: {kernel}, Bandwidth: {bandwidth}')
            plt.axis('off')
    
    plt.tight_layout()
    plt.show()

# Fit KDE with different parameters
kde_results = {}
for kernel in kernels:
    for bandwidth in bandwidths:
        kde = KernelDensity(kernel=kernel, bandwidth=bandwidth)
        kde.fit(encoded_images_pca)
        kde_results[(kernel, bandwidth)] = kde

# Plot KDE results
plot_kde_results(encoded_images_pca, kde_results, bandwidths, kernels)












